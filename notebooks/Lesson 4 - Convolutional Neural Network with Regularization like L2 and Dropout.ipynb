{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries import\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "devices = tf.config.list_physical_devices(\"GPU\")\n",
    "tf.config.experimental.set_memory_growth(devices[0], True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# In last session we have seen model is overfitting i.e. training accuracy is much higher than the validation/test accuracy. To reduce this effect we use technique called regularization. There are different methods of regularization techniques which are as follows:\n",
    "1. Reduce model capsity (Simplify model architecture)\n",
    "2. Add L2 regularization\n",
    "3. Dropout\n",
    "4. Early stopping\n",
    "5. Data augmentation\n",
    "6. Batch normalization (It also normalize data in between layer other than working as form of regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000, 1)\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n",
      "float32\n",
      "(50000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# Let's normalise training and testing dataset but why???\n",
    "# 1. Normally if your features are in different scale that leads to issue in training because neural network will inclined towards with features who has large scale values\n",
    "# 2. Large input values are computationaly expensive and memory hungry\n",
    "# 3. It leads to slower convergences of loss function may create problem in accuracy\n",
    "x_train = (x_train.astype(\"float32\") / 255.0) # Type casting because by default result of it in float64 type\n",
    "x_test = (x_test.astype(\"float32\") / 255.0)\n",
    "\n",
    "print(x_train.dtype)\n",
    "print(x_test.dtype)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                524352    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 619,146\n",
      "Trainable params: 618,698\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Functional API\n",
    "def func_model():\n",
    "    inputs = layers.Input(shape=(32, 32, 3))\n",
    "    x = layers.Conv2D(32, 3, padding=\"same\", kernel_regularizer=regularizers.l2(0.01))(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    \n",
    "    x = layers.Conv2D(64, 3, padding=\"same\", kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    \n",
    "    x = layers.Conv2D(128, 3, padding=\"same\", kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    \n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(64, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(10)(x)\n",
    "    \n",
    "    func_model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    func_model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer=keras.optimizers.Adam(3e-4), metrics=[\"accuracy\"])\n",
    "    func_model.summary()\n",
    "    \n",
    "    return func_model\n",
    "\n",
    "func_model = func_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "343/343 [==============================] - 4s 11ms/step - loss: 3.5197 - accuracy: 0.3238 - val_loss: 3.2996 - val_accuracy: 0.1724\n",
      "Epoch 2/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 2.3201 - accuracy: 0.4335 - val_loss: 1.9592 - val_accuracy: 0.5301\n",
      "Epoch 3/150\n",
      "343/343 [==============================] - 4s 10ms/step - loss: 1.9082 - accuracy: 0.4801 - val_loss: 1.5452 - val_accuracy: 0.6244\n",
      "Epoch 4/150\n",
      "343/343 [==============================] - 4s 12ms/step - loss: 1.6930 - accuracy: 0.5189 - val_loss: 1.4954 - val_accuracy: 0.5983\n",
      "Epoch 5/150\n",
      "343/343 [==============================] - 4s 12ms/step - loss: 1.5803 - accuracy: 0.5344 - val_loss: 1.3855 - val_accuracy: 0.6087\n",
      "Epoch 6/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 1.4998 - accuracy: 0.5532 - val_loss: 1.3539 - val_accuracy: 0.6187\n",
      "Epoch 7/150\n",
      "343/343 [==============================] - 3s 10ms/step - loss: 1.4443 - accuracy: 0.5638 - val_loss: 1.3750 - val_accuracy: 0.6187\n",
      "Epoch 8/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 1.4146 - accuracy: 0.5691 - val_loss: 1.3020 - val_accuracy: 0.6277\n",
      "Epoch 9/150\n",
      "343/343 [==============================] - 4s 12ms/step - loss: 1.3664 - accuracy: 0.5820 - val_loss: 1.1659 - val_accuracy: 0.6805\n",
      "Epoch 10/150\n",
      "343/343 [==============================] - 3s 10ms/step - loss: 1.3329 - accuracy: 0.5937 - val_loss: 1.4109 - val_accuracy: 0.5885\n",
      "Epoch 11/150\n",
      "343/343 [==============================] - 5s 14ms/step - loss: 1.3081 - accuracy: 0.6016 - val_loss: 1.2728 - val_accuracy: 0.6207\n",
      "Epoch 12/150\n",
      "343/343 [==============================] - 4s 11ms/step - loss: 1.2908 - accuracy: 0.6084 - val_loss: 1.1836 - val_accuracy: 0.6465\n",
      "Epoch 13/150\n",
      "343/343 [==============================] - 3s 10ms/step - loss: 1.2722 - accuracy: 0.6163 - val_loss: 1.1062 - val_accuracy: 0.7053\n",
      "Epoch 14/150\n",
      "343/343 [==============================] - 5s 14ms/step - loss: 1.2569 - accuracy: 0.6202 - val_loss: 1.1779 - val_accuracy: 0.6611\n",
      "Epoch 15/150\n",
      "343/343 [==============================] - 4s 13ms/step - loss: 1.2413 - accuracy: 0.6261 - val_loss: 1.1294 - val_accuracy: 0.6924\n",
      "Epoch 16/150\n",
      "343/343 [==============================] - 5s 14ms/step - loss: 1.2289 - accuracy: 0.6336 - val_loss: 1.3424 - val_accuracy: 0.6117\n",
      "Epoch 17/150\n",
      "343/343 [==============================] - 4s 10ms/step - loss: 1.2130 - accuracy: 0.6376 - val_loss: 1.1961 - val_accuracy: 0.6693\n",
      "Epoch 18/150\n",
      "343/343 [==============================] - 4s 11ms/step - loss: 1.2050 - accuracy: 0.6400 - val_loss: 1.2026 - val_accuracy: 0.6623\n",
      "Epoch 19/150\n",
      "343/343 [==============================] - 4s 12ms/step - loss: 1.1844 - accuracy: 0.6496 - val_loss: 1.1059 - val_accuracy: 0.7009\n",
      "Epoch 20/150\n",
      "343/343 [==============================] - 3s 10ms/step - loss: 1.1782 - accuracy: 0.6536 - val_loss: 1.1401 - val_accuracy: 0.6907\n",
      "Epoch 21/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 1.1639 - accuracy: 0.6603 - val_loss: 1.2163 - val_accuracy: 0.6491\n",
      "Epoch 22/150\n",
      "343/343 [==============================] - 4s 12ms/step - loss: 1.1550 - accuracy: 0.6620 - val_loss: 1.2840 - val_accuracy: 0.6448\n",
      "Epoch 23/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 1.1525 - accuracy: 0.6626 - val_loss: 1.3376 - val_accuracy: 0.6437\n",
      "Epoch 24/150\n",
      "343/343 [==============================] - 4s 12ms/step - loss: 1.1346 - accuracy: 0.6696 - val_loss: 1.2687 - val_accuracy: 0.6713\n",
      "Epoch 25/150\n",
      "343/343 [==============================] - 3s 10ms/step - loss: 1.1402 - accuracy: 0.6711 - val_loss: 1.4752 - val_accuracy: 0.5799\n",
      "Epoch 26/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 1.1287 - accuracy: 0.6753 - val_loss: 1.2145 - val_accuracy: 0.6761\n",
      "Epoch 27/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 1.1191 - accuracy: 0.6759 - val_loss: 1.1218 - val_accuracy: 0.6995\n",
      "Epoch 28/150\n",
      "343/343 [==============================] - 4s 12ms/step - loss: 1.1195 - accuracy: 0.6780 - val_loss: 1.1658 - val_accuracy: 0.6847\n",
      "Epoch 29/150\n",
      "343/343 [==============================] - 3s 10ms/step - loss: 1.1065 - accuracy: 0.6825 - val_loss: 1.2674 - val_accuracy: 0.6753\n",
      "Epoch 30/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 1.0963 - accuracy: 0.6867 - val_loss: 1.0278 - val_accuracy: 0.7368\n",
      "Epoch 31/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 1.1021 - accuracy: 0.6852 - val_loss: 1.0978 - val_accuracy: 0.7099\n",
      "Epoch 32/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 1.0939 - accuracy: 0.6884 - val_loss: 1.1417 - val_accuracy: 0.6985\n",
      "Epoch 33/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 1.0777 - accuracy: 0.6969 - val_loss: 1.2221 - val_accuracy: 0.6640\n",
      "Epoch 34/150\n",
      "343/343 [==============================] - 3s 10ms/step - loss: 1.0792 - accuracy: 0.6952 - val_loss: 1.2142 - val_accuracy: 0.6901\n",
      "Epoch 35/150\n",
      "343/343 [==============================] - 4s 12ms/step - loss: 1.0702 - accuracy: 0.6999 - val_loss: 1.0963 - val_accuracy: 0.7128\n",
      "Epoch 36/150\n",
      "343/343 [==============================] - 3s 10ms/step - loss: 1.0580 - accuracy: 0.7039 - val_loss: 1.0351 - val_accuracy: 0.7240\n",
      "Epoch 37/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 1.0589 - accuracy: 0.7021 - val_loss: 0.9968 - val_accuracy: 0.7404\n",
      "Epoch 38/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 1.0547 - accuracy: 0.7098 - val_loss: 1.0485 - val_accuracy: 0.7348\n",
      "Epoch 39/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 1.0492 - accuracy: 0.7083 - val_loss: 1.0253 - val_accuracy: 0.7333\n",
      "Epoch 40/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 1.0466 - accuracy: 0.7125 - val_loss: 1.2102 - val_accuracy: 0.6919\n",
      "Epoch 41/150\n",
      "343/343 [==============================] - 3s 10ms/step - loss: 1.0459 - accuracy: 0.7131 - val_loss: 1.2537 - val_accuracy: 0.6699\n",
      "Epoch 42/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 1.0278 - accuracy: 0.7192 - val_loss: 0.9999 - val_accuracy: 0.7471\n",
      "Epoch 43/150\n",
      "343/343 [==============================] - 3s 10ms/step - loss: 1.0368 - accuracy: 0.7182 - val_loss: 1.1809 - val_accuracy: 0.7064\n",
      "Epoch 44/150\n",
      "343/343 [==============================] - 3s 10ms/step - loss: 1.0213 - accuracy: 0.7229 - val_loss: 0.9419 - val_accuracy: 0.7673\n",
      "Epoch 45/150\n",
      "343/343 [==============================] - 3s 10ms/step - loss: 1.0160 - accuracy: 0.7274 - val_loss: 1.0154 - val_accuracy: 0.7508\n",
      "Epoch 46/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 1.0195 - accuracy: 0.7249 - val_loss: 1.1005 - val_accuracy: 0.7193\n",
      "Epoch 47/150\n",
      "343/343 [==============================] - 3s 10ms/step - loss: 1.0130 - accuracy: 0.7286 - val_loss: 1.1873 - val_accuracy: 0.7087\n",
      "Epoch 48/150\n",
      "343/343 [==============================] - 3s 10ms/step - loss: 1.0106 - accuracy: 0.7286 - val_loss: 1.0084 - val_accuracy: 0.7404\n",
      "Epoch 49/150\n",
      "343/343 [==============================] - 3s 10ms/step - loss: 1.0067 - accuracy: 0.7298 - val_loss: 1.1321 - val_accuracy: 0.7087\n",
      "Epoch 50/150\n",
      "343/343 [==============================] - 3s 10ms/step - loss: 1.0062 - accuracy: 0.7318 - val_loss: 1.1341 - val_accuracy: 0.7068\n",
      "Epoch 51/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.9875 - accuracy: 0.7381 - val_loss: 1.0610 - val_accuracy: 0.7372\n",
      "Epoch 52/150\n",
      "343/343 [==============================] - 3s 10ms/step - loss: 0.9880 - accuracy: 0.7388 - val_loss: 1.0557 - val_accuracy: 0.7368\n",
      "Epoch 53/150\n",
      "343/343 [==============================] - 3s 10ms/step - loss: 0.9894 - accuracy: 0.7412 - val_loss: 1.0692 - val_accuracy: 0.7277\n",
      "Epoch 54/150\n",
      "343/343 [==============================] - 3s 10ms/step - loss: 0.9862 - accuracy: 0.7432 - val_loss: 1.0003 - val_accuracy: 0.7561\n",
      "Epoch 55/150\n",
      "343/343 [==============================] - 3s 10ms/step - loss: 0.9756 - accuracy: 0.7452 - val_loss: 1.1023 - val_accuracy: 0.7188\n",
      "Epoch 56/150\n",
      "343/343 [==============================] - 3s 10ms/step - loss: 0.9699 - accuracy: 0.7497 - val_loss: 1.1770 - val_accuracy: 0.6892\n",
      "Epoch 57/150\n",
      "343/343 [==============================] - 3s 10ms/step - loss: 0.9641 - accuracy: 0.7511 - val_loss: 1.0457 - val_accuracy: 0.7280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.9675 - accuracy: 0.7520 - val_loss: 1.0661 - val_accuracy: 0.7385\n",
      "Epoch 59/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.9657 - accuracy: 0.7524 - val_loss: 1.0832 - val_accuracy: 0.7287\n",
      "Epoch 60/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.9680 - accuracy: 0.7530 - val_loss: 1.0497 - val_accuracy: 0.7381\n",
      "Epoch 61/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.9494 - accuracy: 0.7605 - val_loss: 1.0993 - val_accuracy: 0.7399\n",
      "Epoch 62/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.9543 - accuracy: 0.7585 - val_loss: 1.0412 - val_accuracy: 0.7459\n",
      "Epoch 63/150\n",
      "343/343 [==============================] - 3s 10ms/step - loss: 0.9419 - accuracy: 0.7624 - val_loss: 1.8641 - val_accuracy: 0.5907\n",
      "Epoch 64/150\n",
      "343/343 [==============================] - 3s 10ms/step - loss: 0.9450 - accuracy: 0.7656 - val_loss: 1.2569 - val_accuracy: 0.7119\n",
      "Epoch 65/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.9305 - accuracy: 0.7692 - val_loss: 1.4343 - val_accuracy: 0.6208\n",
      "Epoch 66/150\n",
      "343/343 [==============================] - 4s 11ms/step - loss: 0.9344 - accuracy: 0.7681 - val_loss: 1.2045 - val_accuracy: 0.7163\n",
      "Epoch 67/150\n",
      "343/343 [==============================] - 4s 11ms/step - loss: 0.9316 - accuracy: 0.7725 - val_loss: 1.2606 - val_accuracy: 0.7115\n",
      "Epoch 68/150\n",
      "343/343 [==============================] - 4s 12ms/step - loss: 0.9312 - accuracy: 0.7690 - val_loss: 1.4030 - val_accuracy: 0.6637\n",
      "Epoch 69/150\n",
      "343/343 [==============================] - 4s 12ms/step - loss: 0.9248 - accuracy: 0.7723 - val_loss: 1.1236 - val_accuracy: 0.7377\n",
      "Epoch 70/150\n",
      "343/343 [==============================] - 4s 11ms/step - loss: 0.9120 - accuracy: 0.7776 - val_loss: 1.0277 - val_accuracy: 0.7500\n",
      "Epoch 71/150\n",
      "343/343 [==============================] - 4s 10ms/step - loss: 0.9227 - accuracy: 0.7739 - val_loss: 1.0211 - val_accuracy: 0.7619\n",
      "Epoch 72/150\n",
      "343/343 [==============================] - 4s 11ms/step - loss: 0.9163 - accuracy: 0.7762 - val_loss: 1.1875 - val_accuracy: 0.7125\n",
      "Epoch 73/150\n",
      "343/343 [==============================] - 4s 11ms/step - loss: 0.9101 - accuracy: 0.7772 - val_loss: 1.0863 - val_accuracy: 0.7404\n",
      "Epoch 74/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.9139 - accuracy: 0.7768 - val_loss: 1.1884 - val_accuracy: 0.6983\n",
      "Epoch 75/150\n",
      "343/343 [==============================] - 3s 10ms/step - loss: 0.9044 - accuracy: 0.7813 - val_loss: 1.0429 - val_accuracy: 0.7627\n",
      "Epoch 76/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.9105 - accuracy: 0.7809 - val_loss: 1.1874 - val_accuracy: 0.7256\n",
      "Epoch 77/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.9132 - accuracy: 0.7782 - val_loss: 1.0652 - val_accuracy: 0.7567\n",
      "Epoch 78/150\n",
      "343/343 [==============================] - 3s 10ms/step - loss: 0.9047 - accuracy: 0.7817 - val_loss: 1.0707 - val_accuracy: 0.7449\n",
      "Epoch 79/150\n",
      "343/343 [==============================] - 4s 11ms/step - loss: 0.8933 - accuracy: 0.7849 - val_loss: 1.0915 - val_accuracy: 0.7435\n",
      "Epoch 80/150\n",
      "343/343 [==============================] - 4s 10ms/step - loss: 0.9012 - accuracy: 0.7819 - val_loss: 1.2091 - val_accuracy: 0.7044\n",
      "Epoch 81/150\n",
      "343/343 [==============================] - 4s 12ms/step - loss: 0.8940 - accuracy: 0.7859 - val_loss: 0.9988 - val_accuracy: 0.7724\n",
      "Epoch 82/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8845 - accuracy: 0.7891 - val_loss: 1.0586 - val_accuracy: 0.7677\n",
      "Epoch 83/150\n",
      "343/343 [==============================] - 3s 10ms/step - loss: 0.8861 - accuracy: 0.7880 - val_loss: 0.9870 - val_accuracy: 0.7736\n",
      "Epoch 84/150\n",
      "343/343 [==============================] - 4s 11ms/step - loss: 0.8875 - accuracy: 0.7878 - val_loss: 1.2112 - val_accuracy: 0.7131\n",
      "Epoch 85/150\n",
      "343/343 [==============================] - 4s 11ms/step - loss: 0.8886 - accuracy: 0.7889 - val_loss: 1.2370 - val_accuracy: 0.6912\n",
      "Epoch 86/150\n",
      "343/343 [==============================] - 3s 10ms/step - loss: 0.8774 - accuracy: 0.7905 - val_loss: 1.0460 - val_accuracy: 0.7555\n",
      "Epoch 87/150\n",
      "343/343 [==============================] - 3s 10ms/step - loss: 0.8778 - accuracy: 0.7930 - val_loss: 1.2259 - val_accuracy: 0.7249\n",
      "Epoch 88/150\n",
      "343/343 [==============================] - 3s 10ms/step - loss: 0.8802 - accuracy: 0.7917 - val_loss: 1.4773 - val_accuracy: 0.6583\n",
      "Epoch 89/150\n",
      "343/343 [==============================] - 3s 10ms/step - loss: 0.8786 - accuracy: 0.7930 - val_loss: 1.1282 - val_accuracy: 0.7384\n",
      "Epoch 90/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8704 - accuracy: 0.7960 - val_loss: 1.0607 - val_accuracy: 0.7607\n",
      "Epoch 91/150\n",
      "343/343 [==============================] - 4s 11ms/step - loss: 0.8734 - accuracy: 0.7961 - val_loss: 1.1674 - val_accuracy: 0.7281\n",
      "Epoch 92/150\n",
      "343/343 [==============================] - 4s 11ms/step - loss: 0.8687 - accuracy: 0.7962 - val_loss: 1.3336 - val_accuracy: 0.6751\n",
      "Epoch 93/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8707 - accuracy: 0.7960 - val_loss: 1.0294 - val_accuracy: 0.7721\n",
      "Epoch 94/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8712 - accuracy: 0.7957 - val_loss: 1.3554 - val_accuracy: 0.7057\n",
      "Epoch 95/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8678 - accuracy: 0.7984 - val_loss: 1.0295 - val_accuracy: 0.7672\n",
      "Epoch 96/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8611 - accuracy: 0.8015 - val_loss: 1.3666 - val_accuracy: 0.6984\n",
      "Epoch 97/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8526 - accuracy: 0.8047 - val_loss: 0.9960 - val_accuracy: 0.7800\n",
      "Epoch 98/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8669 - accuracy: 0.7959 - val_loss: 1.1256 - val_accuracy: 0.7516\n",
      "Epoch 99/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8621 - accuracy: 0.8010 - val_loss: 1.1524 - val_accuracy: 0.7380\n",
      "Epoch 100/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8641 - accuracy: 0.8010 - val_loss: 1.1591 - val_accuracy: 0.7365\n",
      "Epoch 101/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8603 - accuracy: 0.8012 - val_loss: 1.1886 - val_accuracy: 0.7249\n",
      "Epoch 102/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8548 - accuracy: 0.8028 - val_loss: 1.3313 - val_accuracy: 0.6945\n",
      "Epoch 103/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8521 - accuracy: 0.8045 - val_loss: 1.1476 - val_accuracy: 0.7456\n",
      "Epoch 104/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8563 - accuracy: 0.8026 - val_loss: 1.2187 - val_accuracy: 0.7164\n",
      "Epoch 105/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8515 - accuracy: 0.8043 - val_loss: 1.0933 - val_accuracy: 0.7396\n",
      "Epoch 106/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8420 - accuracy: 0.8071 - val_loss: 1.1004 - val_accuracy: 0.7651\n",
      "Epoch 107/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8460 - accuracy: 0.8084 - val_loss: 1.1919 - val_accuracy: 0.7059\n",
      "Epoch 108/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8541 - accuracy: 0.8064 - val_loss: 1.0816 - val_accuracy: 0.7597\n",
      "Epoch 109/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8435 - accuracy: 0.8100 - val_loss: 1.0756 - val_accuracy: 0.7535\n",
      "Epoch 110/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8555 - accuracy: 0.8032 - val_loss: 1.0711 - val_accuracy: 0.7680\n",
      "Epoch 111/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8475 - accuracy: 0.8092 - val_loss: 1.1476 - val_accuracy: 0.7501\n",
      "Epoch 112/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8354 - accuracy: 0.8125 - val_loss: 1.2774 - val_accuracy: 0.7320\n",
      "Epoch 113/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8418 - accuracy: 0.8090 - val_loss: 1.3318 - val_accuracy: 0.6981\n",
      "Epoch 114/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8518 - accuracy: 0.8073 - val_loss: 1.0898 - val_accuracy: 0.7492\n",
      "Epoch 115/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8360 - accuracy: 0.8144 - val_loss: 1.1036 - val_accuracy: 0.7581\n",
      "Epoch 116/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8408 - accuracy: 0.8131 - val_loss: 1.1173 - val_accuracy: 0.7459\n",
      "Epoch 117/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8380 - accuracy: 0.8125 - val_loss: 1.8574 - val_accuracy: 0.6337\n",
      "Epoch 118/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8378 - accuracy: 0.8127 - val_loss: 1.1190 - val_accuracy: 0.7621\n",
      "Epoch 119/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8350 - accuracy: 0.8145 - val_loss: 1.2241 - val_accuracy: 0.7287\n",
      "Epoch 120/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8335 - accuracy: 0.8155 - val_loss: 1.4003 - val_accuracy: 0.7027\n",
      "Epoch 121/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8219 - accuracy: 0.8193 - val_loss: 1.0795 - val_accuracy: 0.7525\n",
      "Epoch 122/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8256 - accuracy: 0.8152 - val_loss: 1.3987 - val_accuracy: 0.6695\n",
      "Epoch 123/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8356 - accuracy: 0.8133 - val_loss: 1.1421 - val_accuracy: 0.7608\n",
      "Epoch 124/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8375 - accuracy: 0.8136 - val_loss: 1.1585 - val_accuracy: 0.7316\n",
      "Epoch 125/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8255 - accuracy: 0.8160 - val_loss: 1.0486 - val_accuracy: 0.7708\n",
      "Epoch 126/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8215 - accuracy: 0.8192 - val_loss: 1.1396 - val_accuracy: 0.7295\n",
      "Epoch 127/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8300 - accuracy: 0.8163 - val_loss: 1.0488 - val_accuracy: 0.7720\n",
      "Epoch 128/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8264 - accuracy: 0.8167 - val_loss: 1.1230 - val_accuracy: 0.7593\n",
      "Epoch 129/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8284 - accuracy: 0.8166 - val_loss: 1.2794 - val_accuracy: 0.7376\n",
      "Epoch 130/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8256 - accuracy: 0.8200 - val_loss: 1.1362 - val_accuracy: 0.7495\n",
      "Epoch 131/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8243 - accuracy: 0.8194 - val_loss: 1.1038 - val_accuracy: 0.7531\n",
      "Epoch 132/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8289 - accuracy: 0.8166 - val_loss: 1.3876 - val_accuracy: 0.6831\n",
      "Epoch 133/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8195 - accuracy: 0.8226 - val_loss: 1.1671 - val_accuracy: 0.7493\n",
      "Epoch 134/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8285 - accuracy: 0.8181 - val_loss: 1.0916 - val_accuracy: 0.7636\n",
      "Epoch 135/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8318 - accuracy: 0.8171 - val_loss: 1.1140 - val_accuracy: 0.7516\n",
      "Epoch 136/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8231 - accuracy: 0.8174 - val_loss: 1.1422 - val_accuracy: 0.7448\n",
      "Epoch 137/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8211 - accuracy: 0.8219 - val_loss: 1.4451 - val_accuracy: 0.6839\n",
      "Epoch 138/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8162 - accuracy: 0.8223 - val_loss: 1.1312 - val_accuracy: 0.7532\n",
      "Epoch 139/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8255 - accuracy: 0.8199 - val_loss: 1.1860 - val_accuracy: 0.7389\n",
      "Epoch 140/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8230 - accuracy: 0.8194 - val_loss: 1.0547 - val_accuracy: 0.7735\n",
      "Epoch 141/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8134 - accuracy: 0.8231 - val_loss: 1.2613 - val_accuracy: 0.7189\n",
      "Epoch 142/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8137 - accuracy: 0.8239 - val_loss: 1.0941 - val_accuracy: 0.7588\n",
      "Epoch 143/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8164 - accuracy: 0.8216 - val_loss: 1.1690 - val_accuracy: 0.7511\n",
      "Epoch 144/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8244 - accuracy: 0.8203 - val_loss: 1.1837 - val_accuracy: 0.7393\n",
      "Epoch 145/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8035 - accuracy: 0.8264 - val_loss: 1.0675 - val_accuracy: 0.7733\n",
      "Epoch 146/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8014 - accuracy: 0.8282 - val_loss: 1.3204 - val_accuracy: 0.7255\n",
      "Epoch 147/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8052 - accuracy: 0.8267 - val_loss: 1.1068 - val_accuracy: 0.7653\n",
      "Epoch 148/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8061 - accuracy: 0.8259 - val_loss: 1.1080 - val_accuracy: 0.7571\n",
      "Epoch 149/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8215 - accuracy: 0.8232 - val_loss: 1.1273 - val_accuracy: 0.7469\n",
      "Epoch 150/150\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 0.8086 - accuracy: 0.8268 - val_loss: 1.5146 - val_accuracy: 0.7079\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x149c6ece190>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_model.fit(x_train, y_train, batch_size=124, epochs=150, validation_split=0.15) # As we dropping 0.5 connection so we need to increase epochs size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the by just adding BatchNormalization layer overall test accuracy has increase but our training accuracy has shot to roof i.e. sign of overfitting. We need to handle this issue by regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 0s 5ms/step - loss: 1.5211 - accuracy: 0.7018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5210531949996948, 0.7017999887466431]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_model.evaluate(x_test, y_test, batch_size=124)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
