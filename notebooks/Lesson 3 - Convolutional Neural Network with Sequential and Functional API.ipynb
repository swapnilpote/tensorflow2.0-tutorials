{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries import\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "devices = tf.config.list_physical_devices(\"GPU\")\n",
    "tf.config.experimental.set_memory_growth(devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put plot of cifar images to showcase each category data and another one for limited size to show its size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000, 1)\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n",
      "float32\n",
      "(50000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# Let's normalise training and testing dataset but why???\n",
    "# 1. Normally if your features are in different scale that leads to issue in training because neural network will inclined towards with features who has large scale values\n",
    "# 2. Large input values are computationaly expensive and memory hungry\n",
    "# 3. It leads to slower convergences of loss function may create problem in accuracy\n",
    "x_train = (x_train.astype(\"float32\") / 255.0) # Type casting because by default result of it in float64 type\n",
    "x_test = (x_test.astype(\"float32\") / 255.0)\n",
    "\n",
    "print(x_train.dtype)\n",
    "print(x_test.dtype)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 30, 30, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 13, 13, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                131136    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 225,034\n",
      "Trainable params: 225,034\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Sequential API\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Input(shape=(32, 32, 3))) # height, width, channel\n",
    "model.add(layers.Conv2D(32, (3, 3), padding=\"valid\", activation=\"relu\")) # Number of kernal/Channel with size 3*3\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), padding=\"valid\", activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D())\n",
    "model.add(layers.Conv2D(128, (3, 3), padding=\"valid\", activation=\"relu\"))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation=\"relu\"))\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer=keras.optimizers.Adam(lr=3e-4), metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "343/343 [==============================] - 3s 8ms/step - loss: 1.8024 - accuracy: 0.3452 - val_loss: 1.5990 - val_accuracy: 0.4067\n",
      "Epoch 2/10\n",
      "343/343 [==============================] - 2s 7ms/step - loss: 1.4717 - accuracy: 0.4664 - val_loss: 1.3972 - val_accuracy: 0.5013\n",
      "Epoch 3/10\n",
      "343/343 [==============================] - 2s 7ms/step - loss: 1.3558 - accuracy: 0.5140 - val_loss: 1.3240 - val_accuracy: 0.5257\n",
      "Epoch 4/10\n",
      "343/343 [==============================] - 2s 7ms/step - loss: 1.2640 - accuracy: 0.5485 - val_loss: 1.2875 - val_accuracy: 0.5373\n",
      "Epoch 5/10\n",
      "343/343 [==============================] - 2s 6ms/step - loss: 1.1995 - accuracy: 0.5745 - val_loss: 1.1925 - val_accuracy: 0.5768\n",
      "Epoch 6/10\n",
      "343/343 [==============================] - 2s 7ms/step - loss: 1.1385 - accuracy: 0.6007 - val_loss: 1.1485 - val_accuracy: 0.5963\n",
      "Epoch 7/10\n",
      "343/343 [==============================] - 2s 7ms/step - loss: 1.0846 - accuracy: 0.6196 - val_loss: 1.1143 - val_accuracy: 0.6073\n",
      "Epoch 8/10\n",
      "343/343 [==============================] - 2s 7ms/step - loss: 1.0458 - accuracy: 0.6341 - val_loss: 1.1078 - val_accuracy: 0.6140\n",
      "Epoch 9/10\n",
      "343/343 [==============================] - 2s 6ms/step - loss: 1.0054 - accuracy: 0.6501 - val_loss: 1.0393 - val_accuracy: 0.6385\n",
      "Epoch 10/10\n",
      "343/343 [==============================] - 2s 6ms/step - loss: 0.9764 - accuracy: 0.6607 - val_loss: 1.0197 - val_accuracy: 0.6409\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x197d99887f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Play with batch_size(1024 to 124) to see difference in val_accuracy \n",
    "model.fit(x_train, y_train, batch_size=124, epochs=10, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 0s 4ms/step - loss: 1.0184 - accuracy: 0.6412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.018444538116455, 0.6412000060081482]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test, batch_size=124)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 30, 30, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 30, 30, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 13, 13, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 13, 13, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                131136    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 225,930\n",
      "Trainable params: 225,482\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Functional API\n",
    "def func_model():\n",
    "    inputs = layers.Input(shape=(32, 32, 3))\n",
    "    x = layers.Conv2D(32, 3)(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    \n",
    "    x = layers.Conv2D(64, 3)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    \n",
    "    x = layers.Conv2D(128, 3)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    \n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    outputs = layers.Dense(10)(x)\n",
    "    \n",
    "    func_model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    func_model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer=keras.optimizers.Adam(3e-4), metrics=[\"accuracy\"])\n",
    "    func_model.summary()\n",
    "    \n",
    "    return func_model\n",
    "\n",
    "func_model = func_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "343/343 [==============================] - 3s 9ms/step - loss: 1.4208 - accuracy: 0.4896 - val_loss: 2.5511 - val_accuracy: 0.2395\n",
      "Epoch 2/10\n",
      "343/343 [==============================] - 3s 8ms/step - loss: 1.0339 - accuracy: 0.6357 - val_loss: 1.2763 - val_accuracy: 0.5507\n",
      "Epoch 3/10\n",
      "343/343 [==============================] - 3s 8ms/step - loss: 0.8673 - accuracy: 0.6985 - val_loss: 1.0070 - val_accuracy: 0.6452\n",
      "Epoch 4/10\n",
      "343/343 [==============================] - 3s 8ms/step - loss: 0.7624 - accuracy: 0.7348 - val_loss: 1.1607 - val_accuracy: 0.6068\n",
      "Epoch 5/10\n",
      "343/343 [==============================] - 3s 8ms/step - loss: 0.6716 - accuracy: 0.7685 - val_loss: 0.9196 - val_accuracy: 0.6780\n",
      "Epoch 6/10\n",
      "343/343 [==============================] - 3s 8ms/step - loss: 0.5997 - accuracy: 0.7957 - val_loss: 0.9345 - val_accuracy: 0.6863\n",
      "Epoch 7/10\n",
      "343/343 [==============================] - 3s 8ms/step - loss: 0.5369 - accuracy: 0.8182 - val_loss: 0.9731 - val_accuracy: 0.6729\n",
      "Epoch 8/10\n",
      "343/343 [==============================] - 3s 8ms/step - loss: 0.4772 - accuracy: 0.8383 - val_loss: 0.8722 - val_accuracy: 0.7135\n",
      "Epoch 9/10\n",
      "343/343 [==============================] - 3s 8ms/step - loss: 0.4251 - accuracy: 0.8578 - val_loss: 0.9556 - val_accuracy: 0.6943\n",
      "Epoch 10/10\n",
      "343/343 [==============================] - 3s 8ms/step - loss: 0.3793 - accuracy: 0.8760 - val_loss: 0.9696 - val_accuracy: 0.7037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x197922cc670>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_model.fit(x_train, y_train, batch_size=124, epochs=10, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the by just adding BatchNormalization layer overall test accuracy has increase but our training accuracy has shot to roof i.e. sign of overfitting. We need to handle this issue by regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 0s 4ms/step - loss: 0.9800 - accuracy: 0.6993\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9799597859382629, 0.6992999911308289]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_model.evaluate(x_test, y_test, batch_size=124)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Try few things\n",
    "1. Try different architecture with larger epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
